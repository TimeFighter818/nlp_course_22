{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:14:30.306999Z",
     "iopub.status.busy": "2022-05-20T07:14:30.306408Z",
     "iopub.status.idle": "2022-05-20T07:14:31.998505Z",
     "shell.execute_reply": "2022-05-20T07:14:31.997622Z",
     "shell.execute_reply.started": "2022-05-20T07:14:30.306953Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io \n",
    "import os \n",
    "import sys \n",
    "import requests \n",
    "from collections import OrderedDict \n",
    "import math \n",
    "import random \n",
    "import numpy as np \n",
    "import paddle \n",
    "from paddle.nn import Embedding \n",
    "import paddle.nn.functional as F \n",
    "import paddle.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-20T06:10:31.665048Z",
     "iopub.status.busy": "2022-05-20T06:10:31.664274Z",
     "iopub.status.idle": "2022-05-20T06:10:43.388279Z",
     "shell.execute_reply": "2022-05-20T06:10:43.387517Z",
     "shell.execute_reply.started": "2022-05-20T06:10:31.665005Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download():\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/word2vec/text8.txt\" \n",
    "    web_request = requests.get(corpus_url) \n",
    "    corpus = web_request.content \n",
    "    with open(\"./text8.txt\", \"wb\") as f: \n",
    "        f.write(corpus) \n",
    "    f.close()\n",
    "\n",
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:14:59.053760Z",
     "iopub.status.busy": "2022-05-20T07:14:59.053236Z",
     "iopub.status.idle": "2022-05-20T07:14:59.313328Z",
     "shell.execute_reply": "2022-05-20T07:14:59.312535Z",
     "shell.execute_reply.started": "2022-05-20T07:14:59.053712Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "def load_text8():\n",
    "    with open(\"./text8.txt\", \"r\") as f:\n",
    "        corpus = f.read().strip(\"\\n\") \n",
    "    f.close() \n",
    "    return corpus \n",
    "\n",
    "corpus = load_text8()\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:15:07.333251Z",
     "iopub.status.busy": "2022-05-20T07:15:07.332674Z",
     "iopub.status.idle": "2022-05-20T07:15:08.895611Z",
     "shell.execute_reply": "2022-05-20T07:15:08.894841Z",
     "shell.execute_reply.started": "2022-05-20T07:15:07.333208Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing', 'interpretations', 'of', 'what', 'this', 'means', 'anarchism', 'also', 'refers', 'to', 'related', 'social', 'movements', 'that', 'advocate', 'the', 'elimination', 'of', 'authoritarian', 'institutions', 'particularly', 'the', 'state', 'the', 'word', 'anarchy', 'as', 'most', 'anarchists', 'use', 'it', 'does', 'not', 'imply', 'chaos', 'nihilism', 'or', 'anomie', 'but', 'rather', 'a', 'harmonious', 'anti', 'authoritarian', 'society', 'in', 'place', 'of', 'what', 'are', 'regarded', 'as', 'authoritarian', 'political', 'structures', 'and', 'coercive', 'economic', 'institutions', 'anarchists', 'advocate', 'social', 'relations', 'based', 'upon', 'voluntary', 'association', 'of', 'autonomous', 'individuals', 'mutual', 'aid', 'and', 'self', 'governance', 'while', 'anarchism', 'is', 'most', 'easily', 'defined', 'by', 'what', 'it', 'is', 'against', 'anarchists', 'also', 'offer', 'positive', 'visions', 'of', 'what', 'they', 'believe', 'to', 'be', 'a', 'truly', 'free', 'society', 'however', 'ideas', 'about', 'how', 'an', 'anarchist', 'society', 'might', 'work', 'vary', 'considerably', 'especially', 'with', 'respect', 'to', 'economics', 'there', 'is', 'also', 'disagreement', 'about', 'how', 'a', 'free', 'society', 'might', 'be', 'brought', 'about', 'origins', 'and', 'predecessors', 'kropotkin', 'and', 'others', 'argue', 'that', 'before', 'recorded', 'history', 'human', 'society', 'was', 'organized', 'on', 'anarchist', 'principles', 'most', 'anthropologists', 'follow', 'kropotkin', 'and', 'engels', 'in', 'believing', 'that', 'hunter', 'gatherer', 'bands', 'were', 'egalitarian', 'and', 'lacked', 'division', 'of', 'labour', 'accumulated', 'wealth', 'or', 'decreed', 'law', 'and', 'had', 'equal', 'access', 'to', 'resources', 'william', 'godwin', 'anarchists', 'including', 'the', 'the', 'anarchy', 'organisation', 'and', 'rothbard', 'find', 'anarchist', 'attitudes', 'in', 'taoism', 'from', 'ancient', 'china', 'kropotkin', 'found', 'similar', 'ideas', 'in', 'stoic', 'zeno', 'of', 'citium', 'according', 'to', 'kropotkin', 'zeno', 'repudiated', 'the', 'omnipotence', 'of', 'the', 'state', 'its', 'intervention', 'and', 'regimentation', 'and', 'proclaimed', 'the', 'sovereignty', 'of', 'the', 'moral', 'law', 'of', 'the', 'individual', 'the', 'anabaptists', 'of', 'one', 'six', 'th', 'century', 'europe', 'are', 'sometimes', 'considered', 'to', 'be', 'religious', 'forerunners', 'of', 'modern', 'anarchism', 'bertrand', 'russell', 'in', 'his', 'history', 'of', 'western', 'philosophy', 'writes', 'that', 'the', 'anabaptists', 'repudiated', 'all', 'law', 'since', 'they', 'held', 'that', 'the', 'good', 'man', 'will', 'be', 'guided', 'at', 'every', 'moment', 'by', 'the', 'holy', 'spirit', 'from', 'this', 'premise', 'they', 'arrive', 'at', 'communism', 'the', 'diggers', 'or', 'true', 'levellers', 'were', 'an', 'early', 'communistic', 'movement', 'during', 'the', 'time', 'of', 'the', 'english', 'civil', 'war', 'and', 'are', 'considered', 'by', 'some', 'as', 'forerunners', 'of', 'modern', 'anarchism', 'in', 'the', 'modern', 'era', 'the', 'first', 'to', 'use', 'the', 'term', 'to', 'mean', 'something', 'other', 'than', 'chaos', 'was', 'louis', 'armand', 'baron', 'de', 'lahontan', 'in', 'his', 'nouveaux', 'voyages', 'dans', 'l', 'am', 'rique', 'septentrionale', 'one', 'seven', 'zero', 'three', 'where', 'he', 'described', 'the', 'indigenous', 'american', 'society', 'which', 'had', 'no', 'state', 'laws', 'prisons', 'priests', 'or', 'private', 'property', 'as', 'being', 'in', 'anarchy', 'russell', 'means', 'a', 'libertarian', 'and', 'leader', 'in', 'the', 'american', 'indian', 'movement', 'has', 'repeatedly', 'stated', 'that', 'he', 'is', 'an', 'anarchist', 'and', 'so', 'are', 'all', 'his', 'ancestors', 'in', 'one', 'seven', 'nine', 'three']\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(corpus):\n",
    "    corpus = corpus.strip().lower() \n",
    "    corpus = corpus.split(\" \") \n",
    "    return corpus \n",
    "\n",
    "corpus = data_preprocess(corpus) \n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:15:17.606112Z",
     "iopub.status.busy": "2022-05-20T07:15:17.605289Z",
     "iopub.status.idle": "2022-05-20T07:15:23.928650Z",
     "shell.execute_reply": "2022-05-20T07:15:23.927965Z",
     "shell.execute_reply.started": "2022-05-20T07:15:17.606067Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totally 253854 different words in the corpus\n",
      "word the, its id 0, its word freq 1061396\n",
      "word of, its id 1, its word freq 593677\n",
      "word and, its id 2, its word freq 416629\n",
      "word one, its id 3, its word freq 411764\n",
      "word in, its id 4, its word freq 372201\n",
      "word a, its id 5, its word freq 325873\n",
      "word to, its id 6, its word freq 316376\n",
      "word zero, its id 7, its word freq 264975\n",
      "word nine, its id 8, its word freq 250430\n",
      "word two, its id 9, its word freq 192644\n",
      "word is, its id 10, its word freq 183153\n",
      "word as, its id 11, its word freq 131815\n",
      "word eight, its id 12, its word freq 125285\n",
      "word for, its id 13, its word freq 118445\n",
      "word s, its id 14, its word freq 116710\n",
      "word five, its id 15, its word freq 115789\n",
      "word three, its id 16, its word freq 114775\n",
      "word was, its id 17, its word freq 112807\n",
      "word by, its id 18, its word freq 111831\n",
      "word that, its id 19, its word freq 109510\n",
      "word four, its id 20, its word freq 108182\n",
      "word six, its id 21, its word freq 102145\n",
      "word seven, its id 22, its word freq 99683\n",
      "word with, its id 23, its word freq 95603\n",
      "word on, its id 24, its word freq 91250\n",
      "word are, its id 25, its word freq 76527\n",
      "word it, its id 26, its word freq 73334\n",
      "word from, its id 27, its word freq 72871\n",
      "word or, its id 28, its word freq 68945\n",
      "word his, its id 29, its word freq 62603\n",
      "word an, its id 30, its word freq 61925\n",
      "word be, its id 31, its word freq 61281\n",
      "word this, its id 32, its word freq 58832\n",
      "word which, its id 33, its word freq 54788\n",
      "word at, its id 34, its word freq 54576\n",
      "word he, its id 35, its word freq 53573\n",
      "word also, its id 36, its word freq 44358\n",
      "word not, its id 37, its word freq 44033\n",
      "word have, its id 38, its word freq 39712\n",
      "word were, its id 39, its word freq 39086\n",
      "word has, its id 40, its word freq 37866\n",
      "word but, its id 41, its word freq 35358\n",
      "word other, its id 42, its word freq 32433\n",
      "word their, its id 43, its word freq 31523\n",
      "word its, its id 44, its word freq 29567\n",
      "word first, its id 45, its word freq 28810\n",
      "word they, its id 46, its word freq 28553\n",
      "word some, its id 47, its word freq 28161\n",
      "word had, its id 48, its word freq 28100\n",
      "word all, its id 49, its word freq 26229\n"
     ]
    }
   ],
   "source": [
    "def build_dict(corpus):\n",
    "    word_freq_dict = dict() \n",
    "    for word in corpus: \n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0 \n",
    "        word_freq_dict[word] += 1 \n",
    "    \n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse=True) \n",
    "    \n",
    "    word2id_dict = dict() \n",
    "    word2id_freq = dict() \n",
    "    id2word_dict = dict() \n",
    "\n",
    "    for word, freq in word_freq_dict:\n",
    "        curr_id = len(word2id_dict) \n",
    "        word2id_dict[word] = curr_id \n",
    "        word2id_freq[word2id_dict[word]] = freq \n",
    "        id2word_dict[curr_id] = word \n",
    "    \n",
    "    return word2id_freq, word2id_dict, id2word_dict  \n",
    "\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus) \n",
    "vocab_size = len(word2id_freq) \n",
    "print(\"there are totally %d different words in the corpus\" %vocab_size) \n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:23:59.073519Z",
     "iopub.status.busy": "2022-05-20T07:23:59.072785Z",
     "iopub.status.idle": "2022-05-20T07:24:02.084812Z",
     "shell.execute_reply": "2022-05-20T07:24:02.084072Z",
     "shell.execute_reply.started": "2022-05-20T07:23:59.073471Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580, 0, 194, 10, 190, 58, 4, 5, 10712, 214, 6, 1324, 104, 454, 19, 58, 2731, 362, 6, 3672, 0]\n"
     ]
    }
   ],
   "source": [
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    corpus = [word2id_dict[word] for word in corpus]\n",
    "    return corpus \n",
    "\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict) \n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:24:06.183084Z",
     "iopub.status.busy": "2022-05-20T07:24:06.182578Z",
     "iopub.status.idle": "2022-05-20T07:24:16.887790Z",
     "shell.execute_reply": "2022-05-20T07:24:16.887113Z",
     "shell.execute_reply.started": "2022-05-20T07:24:06.183043Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8746467 tokens in the corpus\n",
      "[5233, 3080, 3133, 741, 476, 10571, 27349, 1, 854, 15067, 58112, 854, 3580, 194, 58, 10712, 1324, 104, 454, 58, 2731, 362, 6, 3672, 708, 539, 11, 1423, 2757, 18, 686, 7088, 5233, 1052, 320, 248, 44611, 2877, 792, 5233, 200, 602, 1134, 2621, 8983, 279, 4147, 141, 6437, 4186]\n"
     ]
    }
   ],
   "source": [
    "def subsampling(corpus, word2id_freq):\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus) \n",
    "        )\n",
    "    corpus = [word for word in corpus if not discard(word)] \n",
    "    return corpus \n",
    "\n",
    "corpus = subsampling(corpus, word2id_freq) \n",
    "print(\"%d tokens in the corpus\" % len(corpus)) \n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:24:19.985336Z",
     "iopub.status.busy": "2022-05-20T07:24:19.984860Z",
     "iopub.status.idle": "2022-05-20T07:25:02.540340Z",
     "shell.execute_reply": "2022-05-20T07:25:02.539414Z",
     "shell.execute_reply.started": "2022-05-20T07:24:19.985300Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_word anarchism, target originated, label 1\n",
      "center_word anarchism, target hydroxyprogesterone, label 0\n",
      "center_word anarchism, target volatilize, label 0\n",
      "center_word anarchism, target drp, label 0\n",
      "center_word anarchism, target maimana, label 0\n",
      "center_word anarchism, target abuse, label 1\n",
      "center_word anarchism, target jondo, label 0\n",
      "center_word anarchism, target chock, label 0\n",
      "center_word anarchism, target bmuld, label 0\n",
      "center_word anarchism, target muirthemne, label 0\n",
      "center_word originated, target anarchism, label 1\n",
      "center_word originated, target electrocutions, label 0\n",
      "center_word originated, target kuperjanov, label 0\n",
      "center_word originated, target pueblos, label 0\n",
      "center_word originated, target planetographic, label 0\n",
      "center_word originated, target abuse, label 1\n",
      "center_word originated, target standardised, label 0\n",
      "center_word originated, target segregationalist, label 0\n",
      "center_word originated, target discontinuous, label 0\n",
      "center_word originated, target mississippian, label 0\n",
      "center_word originated, target working, label 1\n",
      "center_word originated, target pinsker, label 0\n",
      "center_word originated, target able, label 0\n",
      "center_word originated, target behavorial, label 0\n",
      "center_word originated, target tipplers, label 0\n",
      "center_word abuse, target anarchism, label 1\n",
      "center_word abuse, target interactively, label 0\n",
      "center_word abuse, target dethronement, label 0\n",
      "center_word abuse, target ivanchuk, label 0\n",
      "center_word abuse, target freescale, label 0\n",
      "center_word abuse, target originated, label 1\n",
      "center_word abuse, target woodcocks, label 0\n",
      "center_word abuse, target okhranka, label 0\n",
      "center_word abuse, target mufamadi, label 0\n",
      "center_word abuse, target chimp, label 0\n",
      "center_word abuse, target working, label 1\n",
      "center_word abuse, target chlochair, label 0\n",
      "center_word abuse, target kites, label 0\n",
      "center_word abuse, target regardie, label 0\n",
      "center_word abuse, target horsecart, label 0\n",
      "center_word abuse, target class, label 1\n",
      "center_word abuse, target bohn, label 0\n",
      "center_word abuse, target ebon, label 0\n",
      "center_word abuse, target nondiscrete, label 0\n",
      "center_word abuse, target arcipelago, label 0\n",
      "center_word working, target anarchism, label 1\n",
      "center_word working, target prasittisuk, label 0\n",
      "center_word working, target folks, label 0\n",
      "center_word working, target poios, label 0\n",
      "center_word working, target ballum, label 0\n"
     ]
    }
   ],
   "source": [
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, negative_sample_num = 4):\n",
    "\n",
    "    dataset = [] \n",
    "\n",
    "    for center_word_idx in range(len(corpus)):\n",
    "        window_size = random.randint(1, max_window_size) \n",
    "        center_word = corpus[center_word_idx] \n",
    "\n",
    "        positive_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size)) \n",
    "        positive_word_candidates = [corpus[idx] for idx in range(positive_word_range[0], positive_word_range[1]+1) if idx!=center_word_idx] \n",
    "\n",
    "        for positive_word in positive_word_candidates:\n",
    "            dataset.append((center_word, positive_word, 1))\n",
    "        \n",
    "            i = 0 \n",
    "            while i < negative_sample_num:\n",
    "                negative_word_candidate = random.randint(0, vocab_size - 1) \n",
    "                if negative_word_candidate not in positive_word_candidates:\n",
    "                    dataset.append((center_word, negative_word_candidate, 0)) \n",
    "                    i += 1 \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "corpus_light = corpus[:int(len(corpus)*0.2)]\n",
    "dataset = build_data(corpus_light, word2id_dict, word2id_freq) \n",
    "for _, (center_word, target_word, label) in zip(range(50), dataset):\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[center_word], id2word_dict[target_word], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:31:02.575810Z",
     "iopub.status.busy": "2022-05-20T07:31:02.574881Z",
     "iopub.status.idle": "2022-05-20T07:32:07.931464Z",
     "shell.execute_reply": "2022-05-20T07:32:07.930513Z",
     "shell.execute_reply.started": "2022-05-20T07:31:02.575766Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[143978],\n",
      "       [  6194],\n",
      "       [   435],\n",
      "       [   312],\n",
      "       [   585],\n",
      "       [   690],\n",
      "       [  1155],\n",
      "       [   251],\n",
      "       [  1842],\n",
      "       [  4181],\n",
      "       [   990],\n",
      "       [  5724],\n",
      "       [    41],\n",
      "       [  3788],\n",
      "       [     0],\n",
      "       [  1503],\n",
      "       [  2465],\n",
      "       [   376],\n",
      "       [  6976],\n",
      "       [ 33276],\n",
      "       [ 32033],\n",
      "       [   258],\n",
      "       [  1310],\n",
      "       [   394],\n",
      "       [   150],\n",
      "       [    61],\n",
      "       [    42],\n",
      "       [ 52168],\n",
      "       [   826],\n",
      "       [159185],\n",
      "       [  1506],\n",
      "       [   474],\n",
      "       [  6831],\n",
      "       [   771],\n",
      "       [     6],\n",
      "       [  1983],\n",
      "       [  8017],\n",
      "       [  4552],\n",
      "       [   105],\n",
      "       [  1601],\n",
      "       [    59],\n",
      "       [   841],\n",
      "       [  1330],\n",
      "       [  1000],\n",
      "       [  2038],\n",
      "       [  1506],\n",
      "       [   279],\n",
      "       [  1888],\n",
      "       [    65],\n",
      "       [ 10180],\n",
      "       [  3417],\n",
      "       [    71],\n",
      "       [    42],\n",
      "       [   197],\n",
      "       [  1164],\n",
      "       [    66],\n",
      "       [ 23540],\n",
      "       [   390],\n",
      "       [  2133],\n",
      "       [  2918],\n",
      "       [    44],\n",
      "       [  3247],\n",
      "       [    20],\n",
      "       [   173],\n",
      "       [  5187],\n",
      "       [    67],\n",
      "       [   467],\n",
      "       [  2725],\n",
      "       [  2561],\n",
      "       [    56],\n",
      "       [  1510],\n",
      "       [  1252],\n",
      "       [  6456],\n",
      "       [   398],\n",
      "       [  1489],\n",
      "       [  9184],\n",
      "       [147599],\n",
      "       [ 72084],\n",
      "       [  1695],\n",
      "       [   251],\n",
      "       [   188],\n",
      "       [142148],\n",
      "       [  1167],\n",
      "       [    39],\n",
      "       [  4671],\n",
      "       [   511],\n",
      "       [     5],\n",
      "       [    25],\n",
      "       [   238],\n",
      "       [   291],\n",
      "       [  4448],\n",
      "       [  4820],\n",
      "       [ 48159],\n",
      "       [   195],\n",
      "       [   514],\n",
      "       [   977],\n",
      "       [     3],\n",
      "       [   672],\n",
      "       [   240],\n",
      "       [     9],\n",
      "       [   421],\n",
      "       [    27],\n",
      "       [ 15013],\n",
      "       [  4341],\n",
      "       [ 12982],\n",
      "       [  3524],\n",
      "       [   906],\n",
      "       [     7],\n",
      "       [  5277],\n",
      "       [  5813],\n",
      "       [   301],\n",
      "       [   433],\n",
      "       [ 72806],\n",
      "       [  6062],\n",
      "       [ 74702],\n",
      "       [  6267],\n",
      "       [   934],\n",
      "       [ 38391],\n",
      "       [   251],\n",
      "       [  1848],\n",
      "       [    22],\n",
      "       [   740],\n",
      "       [  4601],\n",
      "       [   957],\n",
      "       [   306],\n",
      "       [  1199],\n",
      "       [  1504],\n",
      "       [  5472]]), array([[252804],\n",
      "       [ 12074],\n",
      "       [226765],\n",
      "       [224704],\n",
      "       [ 14129],\n",
      "       [242876],\n",
      "       [224556],\n",
      "       [  1818],\n",
      "       [ 15506],\n",
      "       [ 73843],\n",
      "       [ 85568],\n",
      "       [196614],\n",
      "       [217087],\n",
      "       [142395],\n",
      "       [  2626],\n",
      "       [ 56851],\n",
      "       [   716],\n",
      "       [187544],\n",
      "       [ 15174],\n",
      "       [224591],\n",
      "       [ 54618],\n",
      "       [ 65265],\n",
      "       [101190],\n",
      "       [  2523],\n",
      "       [180164],\n",
      "       [211646],\n",
      "       [ 23028],\n",
      "       [   955],\n",
      "       [230894],\n",
      "       [227542],\n",
      "       [  3291],\n",
      "       [173799],\n",
      "       [234506],\n",
      "       [ 48936],\n",
      "       [222088],\n",
      "       [ 45732],\n",
      "       [195608],\n",
      "       [ 88381],\n",
      "       [250381],\n",
      "       [  8099],\n",
      "       [ 40108],\n",
      "       [246692],\n",
      "       [ 27541],\n",
      "       [  2159],\n",
      "       [248274],\n",
      "       [  8176],\n",
      "       [172529],\n",
      "       [210000],\n",
      "       [237248],\n",
      "       [  2529],\n",
      "       [  5813],\n",
      "       [ 21644],\n",
      "       [ 88690],\n",
      "       [160273],\n",
      "       [ 67758],\n",
      "       [212521],\n",
      "       [    57],\n",
      "       [104715],\n",
      "       [120870],\n",
      "       [ 29177],\n",
      "       [211863],\n",
      "       [ 13660],\n",
      "       [129927],\n",
      "       [ 69484],\n",
      "       [134852],\n",
      "       [ 84475],\n",
      "       [ 47035],\n",
      "       [  6608],\n",
      "       [162335],\n",
      "       [ 85213],\n",
      "       [176291],\n",
      "       [ 80226],\n",
      "       [ 93722],\n",
      "       [  8513],\n",
      "       [164261],\n",
      "       [136848],\n",
      "       [147598],\n",
      "       [134584],\n",
      "       [ 89997],\n",
      "       [154087],\n",
      "       [  5114],\n",
      "       [191740],\n",
      "       [ 74192],\n",
      "       [183921],\n",
      "       [164477],\n",
      "       [  4807],\n",
      "       [   915],\n",
      "       [251931],\n",
      "       [ 19371],\n",
      "       [ 13356],\n",
      "       [184820],\n",
      "       [145264],\n",
      "       [ 60128],\n",
      "       [108582],\n",
      "       [129753],\n",
      "       [172671],\n",
      "       [ 49504],\n",
      "       [ 11709],\n",
      "       [ 52071],\n",
      "       [135468],\n",
      "       [ 50082],\n",
      "       [107762],\n",
      "       [132060],\n",
      "       [201658],\n",
      "       [ 82150],\n",
      "       [182300],\n",
      "       [ 95001],\n",
      "       [  1640],\n",
      "       [155655],\n",
      "       [171165],\n",
      "       [ 21477],\n",
      "       [145675],\n",
      "       [ 84362],\n",
      "       [211879],\n",
      "       [ 28883],\n",
      "       [ 82447],\n",
      "       [162839],\n",
      "       [197586],\n",
      "       [     4],\n",
      "       [109885],\n",
      "       [250555],\n",
      "       [221553],\n",
      "       [205253],\n",
      "       [ 13152],\n",
      "       [223396],\n",
      "       [ 45686],\n",
      "       [210512],\n",
      "       [122494]]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "def build_batch(dataset, batch_size, epoch_num):\n",
    "    center_word_batch = [] \n",
    "    target_word_batch = [] \n",
    "    label_batch = [] \n",
    "    for epoch in range(epoch_num): \n",
    "        random.shuffle(dataset) \n",
    "        for center_word, target_word, label in dataset: \n",
    "            center_word_batch.append([center_word]) \n",
    "            target_word_batch.append([target_word]) \n",
    "            label_batch.append(label) \n",
    "\n",
    "            if len(center_word_batch) == batch_size: \n",
    "                yield np.array(center_word_batch).astype(\"int64\"), np.array(target_word_batch).astype(\"int64\"), np.array(label_batch).astype(\"float32\") \n",
    "                center_word_batch = [] \n",
    "                target_word_batch = [] \n",
    "                label_batch = [] \n",
    "            \n",
    "    if len(center_word_batch) > 0: \n",
    "        yield np.array(center_word_batch).astype(\"int64\"), np.array(target_word_batch).astype(\"int64\"), np.array(label_batch).astype(\"float32\") \n",
    "\n",
    "for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\n",
    "    print(batch) \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:59:24.150007Z",
     "iopub.status.busy": "2022-05-20T07:59:24.149144Z",
     "iopub.status.idle": "2022-05-20T07:59:24.159289Z",
     "shell.execute_reply": "2022-05-20T07:59:24.158585Z",
     "shell.execute_reply.started": "2022-05-20T07:59:24.149941Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\n",
    "        super(SkipGram, self).__init__() \n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding_size = embedding_size \n",
    "        self.embedding = Embedding(\n",
    "            num_embeddings = self.vocab_size, \n",
    "            embedding_dim = self.embedding_size, \n",
    "            weight_attr = paddle.ParamAttr(\n",
    "                initializer = paddle.nn.initializer.Uniform(\n",
    "                    low = -init_scale, \n",
    "                    high = init_scale\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.embedding_out = Embedding(\n",
    "            num_embeddings = self.vocab_size, \n",
    "            embedding_dim = self.embedding_size, \n",
    "            weight_attr = paddle.nn.initializer.Uniform(\n",
    "                low = -init_scale, \n",
    "                high = init_scale\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, center_words, target_words, label):\n",
    "        center_words_emb = self.embedding(center_words) \n",
    "        target_words_emb = self.embedding_out(target_words) \n",
    "        word_sim = paddle.multiply(center_words_emb, target_words_emb) \n",
    "        word_sim = paddle.sum(word_sim, axis=-1) \n",
    "        word_sim = paddle.reshape(word_sim, shape=[-1]) \n",
    "        pred = F.sigmoid(word_sim) \n",
    "        loss = F.binary_cross_entropy_with_logits(word_sim, label) \n",
    "        loss = paddle.mean(loss) \n",
    "        return pred, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T08:12:37.653777Z",
     "iopub.status.busy": "2022-05-20T08:12:37.652886Z",
     "iopub.status.idle": "2022-05-20T08:55:01.001133Z",
     "shell.execute_reply": "2022-05-20T08:55:01.000233Z",
     "shell.execute_reply.started": "2022-05-20T08:12:37.653724Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 16:12:37.662643 11747 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0520 16:12:37.667212 11747 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, loss 0.692\n",
      "step 2000, loss 0.686\n",
      "step 3000, loss 0.628\n",
      "step 4000, loss 0.514\n",
      "step 5000, loss 0.410\n",
      "step 6000, loss 0.286\n",
      "step 7000, loss 0.251\n",
      "step 8000, loss 0.249\n",
      "step 9000, loss 0.265\n",
      "step 10000, loss 0.205\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is fit\n",
      "for word movie, the similar word is encoding\n",
      "for word movie, the similar word is abundant\n",
      "for word movie, the similar word is reached\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is people\n",
      "for word one, the similar word is fleet\n",
      "for word one, the similar word is scholars\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is varying\n",
      "for word chip, the similar word is independence\n",
      "for word chip, the similar word is relations\n",
      "for word chip, the similar word is attend\n",
      "step 11000, loss 0.198\n",
      "step 12000, loss 0.232\n",
      "step 13000, loss 0.193\n",
      "step 14000, loss 0.164\n",
      "step 15000, loss 0.223\n",
      "step 16000, loss 0.235\n",
      "step 17000, loss 0.173\n",
      "step 18000, loss 0.211\n",
      "step 19000, loss 0.145\n",
      "step 20000, loss 0.166\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is jurassic\n",
      "for word movie, the similar word is stability\n",
      "for word movie, the similar word is osmosis\n",
      "for word movie, the similar word is ethical\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is eight\n",
      "for word one, the similar word is four\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is xb\n",
      "for word chip, the similar word is arian\n",
      "for word chip, the similar word is electing\n",
      "for word chip, the similar word is unify\n",
      "step 21000, loss 0.124\n",
      "step 22000, loss 0.218\n",
      "step 23000, loss 0.265\n",
      "step 24000, loss 0.226\n",
      "step 25000, loss 0.193\n",
      "step 26000, loss 0.185\n",
      "step 27000, loss 0.195\n",
      "step 28000, loss 0.151\n",
      "step 29000, loss 0.166\n",
      "step 30000, loss 0.189\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is addressing\n",
      "for word movie, the similar word is footprint\n",
      "for word movie, the similar word is infancy\n",
      "for word movie, the similar word is jurassic\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is eight\n",
      "for word one, the similar word is six\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is four\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is bilinear\n",
      "for word chip, the similar word is videogame\n",
      "for word chip, the similar word is hunnish\n",
      "for word chip, the similar word is garner\n",
      "step 31000, loss 0.205\n",
      "step 32000, loss 0.280\n",
      "step 33000, loss 0.195\n",
      "step 34000, loss 0.195\n",
      "step 35000, loss 0.232\n",
      "step 36000, loss 0.152\n",
      "step 37000, loss 0.201\n",
      "step 38000, loss 0.148\n",
      "step 39000, loss 0.231\n",
      "step 40000, loss 0.178\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is footprint\n",
      "for word movie, the similar word is merchandise\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is embarking\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is eight\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is dualist\n",
      "for word chip, the similar word is individualized\n",
      "for word chip, the similar word is boomers\n",
      "for word chip, the similar word is elliot\n",
      "step 41000, loss 0.133\n",
      "step 42000, loss 0.242\n",
      "step 43000, loss 0.201\n",
      "step 44000, loss 0.230\n",
      "step 45000, loss 0.148\n",
      "step 46000, loss 0.220\n",
      "step 47000, loss 0.266\n",
      "step 48000, loss 0.146\n",
      "step 49000, loss 0.156\n",
      "step 50000, loss 0.192\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is novel\n",
      "for word movie, the similar word is caine\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is six\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is faster\n",
      "for word chip, the similar word is models\n",
      "for word chip, the similar word is ram\n",
      "for word chip, the similar word is device\n",
      "step 51000, loss 0.236\n",
      "step 52000, loss 0.216\n",
      "step 53000, loss 0.165\n",
      "step 54000, loss 0.170\n",
      "step 55000, loss 0.220\n",
      "step 56000, loss 0.205\n",
      "step 57000, loss 0.286\n",
      "step 58000, loss 0.146\n",
      "step 59000, loss 0.170\n",
      "step 60000, loss 0.208\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is novels\n",
      "for word movie, the similar word is novel\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is five\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is faster\n",
      "for word chip, the similar word is machines\n",
      "for word chip, the similar word is chips\n",
      "step 61000, loss 0.123\n",
      "step 62000, loss 0.187\n",
      "step 63000, loss 0.263\n",
      "step 64000, loss 0.165\n",
      "step 65000, loss 0.178\n",
      "step 66000, loss 0.224\n",
      "step 67000, loss 0.186\n",
      "step 68000, loss 0.264\n",
      "step 69000, loss 0.129\n",
      "step 70000, loss 0.164\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is novels\n",
      "for word movie, the similar word is films\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is five\n",
      "for word one, the similar word is nine\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is models\n",
      "for word chip, the similar word is kb\n",
      "for word chip, the similar word is device\n",
      "step 71000, loss 0.146\n",
      "step 72000, loss 0.139\n",
      "step 73000, loss 0.151\n",
      "step 74000, loss 0.165\n",
      "step 75000, loss 0.151\n",
      "step 76000, loss 0.083\n",
      "step 77000, loss 0.185\n",
      "step 78000, loss 0.163\n",
      "step 79000, loss 0.177\n",
      "step 80000, loss 0.155\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is marple\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is actress\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is conductor\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is ram\n",
      "for word chip, the similar word is hardware\n",
      "for word chip, the similar word is adapter\n",
      "step 81000, loss 0.161\n",
      "step 82000, loss 0.120\n",
      "step 83000, loss 0.124\n",
      "step 84000, loss 0.173\n",
      "step 85000, loss 0.125\n",
      "step 86000, loss 0.154\n",
      "step 87000, loss 0.169\n",
      "step 88000, loss 0.224\n",
      "step 89000, loss 0.199\n",
      "step 90000, loss 0.170\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is marple\n",
      "for word movie, the similar word is thriller\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is tre\n",
      "for word one, the similar word is amin\n",
      "for word one, the similar word is semanticist\n",
      "for word one, the similar word is yorker\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is rgb\n",
      "for word chip, the similar word is portable\n",
      "for word chip, the similar word is hardware\n",
      "for word chip, the similar word is unix\n",
      "step 91000, loss 0.127\n",
      "step 92000, loss 0.101\n",
      "step 93000, loss 0.127\n",
      "step 94000, loss 0.163\n",
      "step 95000, loss 0.186\n",
      "step 96000, loss 0.107\n",
      "step 97000, loss 0.144\n",
      "step 98000, loss 0.155\n",
      "step 99000, loss 0.104\n",
      "step 100000, loss 0.126\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is movies\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is thriller\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is mcd\n",
      "for word one, the similar word is championships\n",
      "for word one, the similar word is toppers\n",
      "for word one, the similar word is yorker\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is hardware\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is clock\n",
      "for word chip, the similar word is upgrades\n",
      "step 101000, loss 0.090\n",
      "step 102000, loss 0.174\n",
      "step 103000, loss 0.161\n",
      "step 104000, loss 0.125\n",
      "step 105000, loss 0.172\n",
      "step 106000, loss 0.120\n",
      "step 107000, loss 0.106\n",
      "step 108000, loss 0.127\n",
      "step 109000, loss 0.153\n",
      "step 110000, loss 0.137\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is movies\n",
      "for word movie, the similar word is fantasy\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is yorker\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is yussuf\n",
      "for word one, the similar word is reiner\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is hardware\n",
      "for word chip, the similar word is adapter\n",
      "for word chip, the similar word is clock\n",
      "step 111000, loss 0.158\n",
      "step 112000, loss 0.127\n",
      "step 113000, loss 0.139\n",
      "step 114000, loss 0.122\n",
      "step 115000, loss 0.166\n",
      "step 116000, loss 0.147\n",
      "step 117000, loss 0.108\n",
      "step 118000, loss 0.178\n",
      "step 119000, loss 0.187\n",
      "step 120000, loss 0.162\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is movies\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is tenth\n",
      "for word one, the similar word is superstar\n",
      "for word one, the similar word is alicante\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is firmware\n",
      "for word chip, the similar word is adapter\n",
      "step 121000, loss 0.130\n",
      "step 122000, loss 0.140\n",
      "step 123000, loss 0.196\n",
      "step 124000, loss 0.223\n",
      "step 125000, loss 0.183\n",
      "step 126000, loss 0.163\n",
      "step 127000, loss 0.131\n",
      "step 128000, loss 0.138\n",
      "step 129000, loss 0.120\n",
      "step 130000, loss 0.187\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is broadway\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is cavendish\n",
      "for word one, the similar word is edna\n",
      "for word one, the similar word is speyer\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is adapter\n",
      "for word chip, the similar word is coprocessor\n",
      "step 131000, loss 0.119\n",
      "step 132000, loss 0.150\n",
      "step 133000, loss 0.112\n",
      "step 134000, loss 0.177\n",
      "step 135000, loss 0.189\n",
      "step 136000, loss 0.162\n",
      "step 137000, loss 0.175\n",
      "step 138000, loss 0.064\n",
      "step 139000, loss 0.060\n",
      "step 140000, loss 0.077\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is broadway\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is reiner\n",
      "for word one, the similar word is cavendish\n",
      "for word one, the similar word is superstar\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is upgrades\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is hardware\n",
      "step 141000, loss 0.075\n",
      "step 142000, loss 0.111\n",
      "step 143000, loss 0.111\n",
      "step 144000, loss 0.114\n",
      "step 145000, loss 0.058\n",
      "step 146000, loss 0.066\n",
      "step 147000, loss 0.117\n",
      "step 148000, loss 0.044\n",
      "step 149000, loss 0.118\n",
      "step 150000, loss 0.092\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is howerd\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is archibald\n",
      "for word one, the similar word is superstar\n",
      "for word one, the similar word is mcd\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is coprocessor\n",
      "for word chip, the similar word is upgrades\n",
      "for word chip, the similar word is ula\n",
      "step 151000, loss 0.067\n",
      "step 152000, loss 0.096\n",
      "step 153000, loss 0.079\n",
      "step 154000, loss 0.118\n",
      "step 155000, loss 0.105\n",
      "step 156000, loss 0.050\n",
      "step 157000, loss 0.059\n",
      "step 158000, loss 0.070\n",
      "step 159000, loss 0.127\n",
      "step 160000, loss 0.071\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is marple\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is archibald\n",
      "for word one, the similar word is toppers\n",
      "for word one, the similar word is gasproof\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is coprocessor\n",
      "for word chip, the similar word is scsi\n",
      "step 161000, loss 0.064\n",
      "step 162000, loss 0.073\n",
      "step 163000, loss 0.075\n",
      "step 164000, loss 0.106\n",
      "step 165000, loss 0.071\n",
      "step 166000, loss 0.046\n",
      "step 167000, loss 0.063\n",
      "step 168000, loss 0.083\n",
      "step 169000, loss 0.075\n",
      "step 170000, loss 0.091\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is marple\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is toppers\n",
      "for word one, the similar word is gasproof\n",
      "for word one, the similar word is gibson\n",
      "for word one, the similar word is lockdown\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is coprocessor\n",
      "for word chip, the similar word is upgrades\n",
      "step 171000, loss 0.089\n",
      "step 172000, loss 0.127\n",
      "step 173000, loss 0.081\n",
      "step 174000, loss 0.146\n",
      "step 175000, loss 0.081\n",
      "step 176000, loss 0.168\n",
      "step 177000, loss 0.083\n",
      "step 178000, loss 0.079\n",
      "step 179000, loss 0.096\n",
      "step 180000, loss 0.091\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is fantasy\n",
      "for word movie, the similar word is howerd\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is gibson\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is sci\n",
      "for word one, the similar word is semanticist\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is coprocessor\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is cascade\n",
      "step 181000, loss 0.066\n",
      "step 182000, loss 0.077\n",
      "step 183000, loss 0.082\n",
      "step 184000, loss 0.091\n",
      "step 185000, loss 0.135\n",
      "step 186000, loss 0.129\n",
      "step 187000, loss 0.099\n",
      "step 188000, loss 0.076\n",
      "step 189000, loss 0.096\n",
      "step 190000, loss 0.087\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is parodies\n",
      "for word movie, the similar word is fantasy\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is speyer\n",
      "for word one, the similar word is lockdown\n",
      "for word one, the similar word is edna\n",
      "for word one, the similar word is jesuit\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is coprocessor\n",
      "for word chip, the similar word is clock\n",
      "step 191000, loss 0.071\n",
      "step 192000, loss 0.113\n",
      "step 193000, loss 0.076\n",
      "step 194000, loss 0.096\n",
      "step 195000, loss 0.078\n",
      "step 196000, loss 0.186\n",
      "step 197000, loss 0.108\n",
      "step 198000, loss 0.082\n",
      "step 199000, loss 0.112\n",
      "step 200000, loss 0.141\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is film\n",
      "for word movie, the similar word is thriller\n",
      "for word movie, the similar word is ongais\n",
      "for word movie, the similar word is fantasy\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is edna\n",
      "for word one, the similar word is lockdown\n",
      "for word one, the similar word is gordon\n",
      "for word one, the similar word is dietrich\n",
      "for word chip, the similar word is chip\n",
      "for word chip, the similar word is ula\n",
      "for word chip, the similar word is unix\n",
      "for word chip, the similar word is cascade\n",
      "for word chip, the similar word is coprocessor\n",
      "step 201000, loss 0.182\n",
      "step 202000, loss 0.144\n",
      "step 203000, loss 0.051\n",
      "step 204000, loss 0.111\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512 \n",
    "epoch_num = 3 \n",
    "embedding_size = 200 \n",
    "step = 0 \n",
    "learning_rate = 1e-3\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed): \n",
    "    W = embed.numpy() \n",
    "    x = W[word2id_dict[query_token]] \n",
    "    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten() \n",
    "    indices = np.argpartition(flat, -k)[-k:] \n",
    "    indices = indices[np.argsort(-flat[indices])] \n",
    "    for i in indices: \n",
    "        print('for word %s, the similar word is %s' % (query_token, str(id2word_dict[i]))) \n",
    "\n",
    "skip_gram_model = SkipGram(vocab_size, embedding_size) \n",
    "adam = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=skip_gram_model.parameters()) \n",
    "\n",
    "for center_words, target_words, label in build_batch(dataset, batch_size, epoch_num):\n",
    "    center_words_var = paddle.to_tensor(center_words) \n",
    "    target_words_var = paddle.to_tensor(target_words) \n",
    "    label_var = paddle.to_tensor(label) \n",
    "\n",
    "    pred, loss = skip_gram_model(center_words_var, target_words_var, label_var) \n",
    "    loss.backward() \n",
    "    adam.step() \n",
    "    adam.clear_grad() \n",
    "\n",
    "    step += 1 \n",
    "    if step % 1000 == 0:\n",
    "        print(\"step %d, loss %.3f\" % (step, loss.numpy()[0])) \n",
    "    if step % 10000 == 0:\n",
    "        get_similar_tokens('movie', 5, skip_gram_model.embedding.weight) \n",
    "        get_similar_tokens('one', 5, skip_gram_model.embedding.weight) \n",
    "        get_similar_tokens('chip', 5, skip_gram_model.embedding.weight)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T09:17:18.224024Z",
     "iopub.status.busy": "2022-05-20T09:17:18.223428Z",
     "iopub.status.idle": "2022-05-20T09:17:18.230352Z",
     "shell.execute_reply": "2022-05-20T09:17:18.229511Z",
     "shell.execute_reply.started": "2022-05-20T09:17:18.223971Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from itertools import chain \n",
    "import numpy as np \n",
    "import os \n",
    "import time \n",
    "from multiprocessing import pool, cpu_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CoOccur:\n",
    "    def __init__(self):\n",
    "        self.cooccur = {} \n",
    "    \n",
    "    def pair(self, w1, w2, dis):\n",
    "        if w1 in self.cooccur.keys():\n",
    "            if w2 in self.cooccur[w1].keys():\n",
    "                self.cooccur[w1][w2] += 1.0 / dis \n",
    "            else:\n",
    "                self.cooccur[w1][w2] = 1.0 / dis \n",
    "        else:\n",
    "            self.cooccur[w1] = {} \n",
    "            self.cooccur[w1][w2] = 1.0 / dis \n",
    "    \n",
    "    def check(self, w1, w2):\n",
    "        if w1 in self.cooccur.keys():\n",
    "            if w2 in self.cooccur[w1].keys():\n",
    "                return self.cooccur[w1][w2] \n",
    "        else:\n",
    "            self.cooccur[w1] = {} \n",
    "            self.cooccur[w1][w2] = 1.0 / dis \n",
    "    \n",
    "    def get_pairs(self):\n",
    "        if self.num_saved > 0: \n",
    "            for i in range(1, self.num_saved + 1):\n",
    "                f = open(self.cache_path + '/buffer2bin_' + str(i) + '.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T09:29:06.488240Z",
     "iopub.status.busy": "2022-05-20T09:29:06.487727Z",
     "iopub.status.idle": "2022-05-20T09:30:12.858326Z",
     "shell.execute_reply": "2022-05-20T09:30:12.857637Z",
     "shell.execute_reply.started": "2022-05-20T09:29:06.488190Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 694483/694483 [00:44<00:00, 15677.96it/s]\n",
      "[2022-05-20 17:30:04,131] [    INFO] - Loading token embedding...\n",
      "W0520 17:30:08.293259 20581 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0520 17:30:08.297292 20581 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.\n",
      "[2022-05-20 17:30:12,404] [    INFO] - Finish loading embedding vector.\n",
      "[2022-05-20 17:30:12,407] [    INFO] - Token Embedding info:             \n",
      "Unknown index: 635963             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 635964             \n",
      "Padding token: [PAD]             \n",
      "Shape :[635965, 300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object   type: TokenEmbedding(635965, 300, padding_idx=635964, sparse=False)             \n",
      "Unknown index: 635963             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 635964             \n",
      "Padding token: [PAD]             \n",
      "Parameter containing:\n",
      "Tensor(shape=[635965, 300], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[-0.24200200,  0.13931701,  0.07378800, ...,  0.14103900,\n",
      "          0.05592300, -0.08004800],\n",
      "        [-0.08671700,  0.07770800,  0.09515300, ...,  0.11196400,\n",
      "          0.03082200, -0.12893000],\n",
      "        [-0.11436500,  0.12201900,  0.02833000, ...,  0.11068700,\n",
      "          0.03607300, -0.13763499],\n",
      "        ...,\n",
      "        [ 0.02628800, -0.00008300, -0.00393500, ...,  0.00654000,\n",
      "          0.00024600, -0.00662600],\n",
      "        [ 0.00930271, -0.00925986, -0.00438002, ...,  0.01524009,\n",
      "         -0.00773855,  0.00857220],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "          0.        ,  0.        ]])\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.embeddings import TokenEmbedding \r\n",
    "\r\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\") \r\n",
    "\r\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T10:37:00.721948Z",
     "iopub.status.busy": "2022-05-20T10:37:00.721302Z",
     "iopub.status.idle": "2022-05-20T10:37:02.997799Z",
     "shell.execute_reply": "2022-05-20T10:37:02.996650Z",
     "shell.execute_reply.started": "2022-05-20T10:37:00.721893Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: visualdl\n",
      "Version: 2.2.3\n",
      "Summary: Visualize Deep Learning\n",
      "Home-page: UNKNOWN\n",
      "Author: PaddlePaddle and Echarts team\n",
      "Author-email: \n",
      "License: Apache License\n",
      "Location: /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages\n",
      "Requires: bce-python-sdk, flake8, flask, Flask-Babel, matplotlib, numpy, pandas, Pillow, pre-commit, protobuf, requests, shellcheck-py, six\n",
      "Required-by: paddlehub\n"
     ]
    }
   ],
   "source": [
    "!pip show visualdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T09:31:22.690871Z",
     "iopub.status.busy": "2022-05-20T09:31:22.690009Z",
     "iopub.status.idle": "2022-05-20T09:31:22.699566Z",
     "shell.execute_reply": "2022-05-20T09:31:22.698818Z",
     "shell.execute_reply.started": "2022-05-20T09:31:22.690834Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.260801  0.1047    0.129453 -0.257317 -0.16152   0.19567  -0.074868\n",
      "   0.361168  0.245882 -0.219141 -0.388083  0.235189  0.029316  0.154215\n",
      "  -0.354343  0.017746  0.009028  0.01197  -0.121429  0.096542  0.009255\n",
      "   0.039721  0.363704 -0.239497 -0.41168   0.16958   0.261758  0.022383\n",
      "  -0.053248 -0.000994 -0.209913 -0.208296  0.197332 -0.3426   -0.162112\n",
      "   0.134557 -0.250201  0.431298  0.303116  0.517221  0.243843  0.022219\n",
      "  -0.136554 -0.189223  0.148563 -0.042963 -0.456198  0.14546  -0.041207\n",
      "   0.049685  0.20294   0.147355 -0.206953 -0.302796 -0.111834  0.128183\n",
      "   0.289539 -0.298934 -0.096412  0.063079  0.324821 -0.144471  0.052456\n",
      "   0.088761 -0.040925 -0.103281 -0.216065 -0.200878 -0.100664  0.170614\n",
      "  -0.355546 -0.062115 -0.52595  -0.235442  0.300866 -0.521523 -0.070713\n",
      "  -0.331768  0.023021  0.309111 -0.125696  0.016723 -0.0321   -0.200611\n",
      "   0.057294 -0.128891 -0.392886  0.423002  0.282569 -0.212836  0.450132\n",
      "   0.067604 -0.124928 -0.294086  0.136479  0.091505 -0.061723 -0.577495\n",
      "   0.293856 -0.401198  0.302559 -0.467656  0.021708 -0.088507  0.088322\n",
      "  -0.015567  0.136594  0.112152  0.005394  0.133818  0.071278 -0.198807\n",
      "   0.043538  0.116647 -0.210486 -0.217972 -0.320675  0.293977  0.277564\n",
      "   0.09591  -0.359836  0.473573  0.083847  0.240604  0.441624  0.087959\n",
      "   0.064355 -0.108271  0.055709  0.380487 -0.045262  0.04014  -0.259215\n",
      "  -0.398335  0.52712  -0.181298  0.448978 -0.114245 -0.028225 -0.146037\n",
      "   0.347414 -0.076505  0.461865 -0.105099  0.131892  0.079946  0.32422\n",
      "  -0.258629  0.05225   0.566337  0.348371  0.124111  0.229154  0.075039\n",
      "  -0.139532 -0.08839  -0.026703 -0.222828 -0.106018  0.324477  0.128269\n",
      "  -0.045624  0.071815 -0.135702  0.261474  0.297334 -0.031481  0.18959\n",
      "   0.128716  0.090022  0.037609 -0.049669  0.092909  0.0564   -0.347994\n",
      "  -0.367187 -0.292187  0.021649 -0.102004 -0.398568 -0.278248 -0.082361\n",
      "  -0.161823  0.044846  0.212597 -0.013164  0.005527 -0.004024  0.176243\n",
      "   0.237274 -0.174856 -0.197214  0.150825 -0.164427 -0.244255 -0.14897\n",
      "   0.098907 -0.295891 -0.013408 -0.146875 -0.126049  0.033235 -0.133444\n",
      "  -0.003258  0.082053 -0.162569  0.283657  0.315608 -0.171281 -0.276051\n",
      "   0.258458  0.214045 -0.129798 -0.511728  0.198481 -0.35632  -0.186253\n",
      "  -0.203719  0.22004  -0.016474  0.080321 -0.463004  0.290794 -0.003445\n",
      "   0.061247 -0.069157 -0.022525  0.13514   0.001354  0.011079  0.014223\n",
      "  -0.079145 -0.41402  -0.404242 -0.301509  0.036712  0.037076 -0.061683\n",
      "  -0.202429  0.130216  0.054355  0.140883 -0.030627 -0.281293 -0.28059\n",
      "  -0.214048 -0.467033  0.203632 -0.541544  0.183898 -0.129535 -0.286422\n",
      "  -0.162222  0.262487  0.450505  0.11551  -0.247965 -0.15837   0.060613\n",
      "  -0.285358  0.498203  0.025008 -0.256397  0.207582  0.166383  0.669677\n",
      "  -0.067961 -0.049835 -0.444369  0.369306  0.134493 -0.080478 -0.304565\n",
      "  -0.091756  0.053657  0.114497 -0.076645 -0.123933  0.168645  0.018987\n",
      "  -0.260592 -0.019668 -0.063312 -0.094939  0.657352  0.247547 -0.161621\n",
      "   0.289043 -0.284084  0.205076  0.059885  0.055871  0.159309  0.062181\n",
      "   0.123634  0.282932  0.140399 -0.076253 -0.087103  0.07262 ]]\n"
     ]
    }
   ],
   "source": [
    "test_token_embedding = token_embedding.search(\"\") \r\n",
    "print(test_token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T09:32:32.528581Z",
     "iopub.status.busy": "2022-05-20T09:32:32.527556Z",
     "iopub.status.idle": "2022-05-20T09:32:32.539267Z",
     "shell.execute_reply": "2022-05-20T09:32:32.538532Z",
     "shell.execute_reply.started": "2022-05-20T09:32:32.528527Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1: 0.7017183\n",
      "socre2: 0.19189896\n"
     ]
    }
   ],
   "source": [
    "score1 = token_embedding.cosine_sim(\"\", \"\")\r\n",
    "score2 = token_embedding.cosine_sim(\"\", \"\") \r\n",
    "print(\"score1:\", score1) \r\n",
    "print(\"socre2:\", score2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T10:33:51.034894Z",
     "iopub.status.busy": "2022-05-20T10:33:51.033969Z",
     "iopub.status.idle": "2022-05-20T10:33:51.044311Z",
     "shell.execute_reply": "2022-05-20T10:33:51.043099Z",
     "shell.execute_reply.started": "2022-05-20T10:33:51.034827Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = token_embedding.vocab.to_tokens(list(range(0, 1))) \r\n",
    "test_token_embedding = token_embedding.search(labels) \r\n",
    "from visualdl import LogWriter \r\n",
    "with LogWriter(logdir='./token_hidi') as writer: \r\n",
    "    writer.add_embeddings(tag=\"test\", mat=[i for i in test_token_embedding], metadata=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
