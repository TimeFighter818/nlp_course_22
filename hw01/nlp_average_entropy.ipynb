{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d406a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import jieba\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b1c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseData:\n",
    "    def __init__(self, txtname='', txt='', sentences=[], words=[], entropyinit = {}):\n",
    "        self.txtname = txtname\n",
    "        self.txt = txt\n",
    "        self.sentences = sentences\n",
    "        self.words = words\n",
    "        global punctuation\n",
    "        self.punctuation = punctuation\n",
    "        global stopwords\n",
    "        self.stopwords = stopwords\n",
    "        self.entropy = entropyinit\n",
    "\n",
    "    def sepSentences(self):\n",
    "        line = ''\n",
    "        sentences = []\n",
    "        for w in self.txt:\n",
    "            if w in self.punctuation and line != '\\n':\n",
    "                if line.strip() != '':\n",
    "                    sentences.append(line.strip())\n",
    "                    line = ''\n",
    "            elif w not in self.punctuation:\n",
    "                line += w\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def sepWords(self):\n",
    "        words = []\n",
    "        dete_stopwords = 0\n",
    "        if dete_stopwords:\n",
    "            for i in range(len(self.sentences)):\n",
    "                words.extend([x for x in jieba.cut(self.sentences[i]) if x not in self.stopwords])\n",
    "        else:\n",
    "            for i in range(len(self.sentences)):\n",
    "                words.extend([x for x in jieba.cut(self.sentences[i])])\n",
    "        self.words = words\n",
    "    \n",
    "    def getNmodel(self, phrase_model, n):\n",
    "        if n == 1:\n",
    "            for i in range(len(self.words)):\n",
    "                phrase_model[self.words[i]] = phrase_model.get(self.words[i], 0) + 1\n",
    "        else:\n",
    "            for i in range(len(self.words) - (n - 1)):\n",
    "                if n == 2:\n",
    "                    condition_t = self.words[i]\n",
    "                else:\n",
    "                    condition = []\n",
    "                    for j in range(n-1):\n",
    "                        condition.append(self.words[i + j])\n",
    "                    condition_t = tuple(condition)\n",
    "                phrase_model[(condition_t, self.words[i+n-1])] = phrase_model.get((condition_t, self.words[i+n-1]), 0) + 1\n",
    "    \n",
    "    def getN_1model(self, phrase_model, n):\n",
    "        if n == 1:\n",
    "            for i in range(len(self.words)):\n",
    "                phrase_model[self.words[i]] = phrase_model.get(self.words[i], 0) + 1\n",
    "        else:\n",
    "            for i in range(len(self.words) - (n - 1)):\n",
    "                condition = []\n",
    "                for j in range(n):\n",
    "                    condition.append(self.words[i + j])\n",
    "                condition_t = tuple(condition)\n",
    "                phrase_model[condition_t] = phrase_model.get(condition_t, 0) + 1\n",
    "                                \n",
    "    def calcuNmodelEntropy(self, n, entropy_dic):\n",
    "        if n < 1 or n >= len(self.words):\n",
    "            print(\"Wrong N!\")\n",
    "        elif n == 1:\n",
    "            phrase_model = {}\n",
    "            self.getNmodel(phrase_model, 1)\n",
    "            model_len = len(self.words)\n",
    "            entropy_dic[n] = sum([-(phrase[1] / model_len * math.log(phrase[1] / model_len, 2)) for phrase in phrase_model.items()])\n",
    "            entropy_dic[n] = round(entropy_dic[n], 4) \n",
    "            # self.entropy[n] = sum([-(phrase[1] / model_lenth) * math.log(phrase[1] / model_lenth, 2) for phrase in phrase_model.items()])\n",
    "            # self.entropy[n] = round(self.entropy[n], 4)  \n",
    "            # self.entropy.append(sum([-(phrase[1] / model_lenth) * math.log(phrase[1] / model_lenth, 2) for phrase in phrase_model.items()]))         \n",
    "            # self.entropy = sum([-(phrase[1] / model_lenth) * math.log(phrase[1] / model_lenth, 2) for phrase in phrase_model.items()])\n",
    "        else:\n",
    "            phrase_model_n = {}\n",
    "            phrase_model_n_1 = {}\n",
    "            self.getNmodel(phrase_model_n, n)\n",
    "            self.getN_1model(phrase_model_n_1, n - 1)\n",
    "            phrase_n_len = sum([phrase[1] for phrase in phrase_model_n.items()])\n",
    "            entropy = []\n",
    "            for n_phrase in phrase_model_n.items():\n",
    "                p_xy = n_phrase[1] / phrase_n_len\n",
    "                p_x_y = n_phrase[1] / phrase_model_n_1[n_phrase[0][0]] \n",
    "                entropy.append(-p_xy * math.log(p_x_y, 2))\n",
    "            entropy_dic[n] = round(sum(entropy), 4)\n",
    "            # self.entropy[n] = round(sum(entropy), 4)\n",
    "            # self.entropy.append(round(sum(entropy), 4))\n",
    "            # self.entropy = round(sum(entropy), 4)\n",
    " \n",
    "    def run(self):\n",
    "        self.sepSentences()\n",
    "        self.sepWords()\n",
    "        entropy_dic = {}\n",
    "        self.calcuNmodelEntropy(1, entropy_dic)\n",
    "        self.calcuNmodelEntropy(2, entropy_dic)\n",
    "        self.calcuNmodelEntropy(3, entropy_dic)\n",
    "        self.calcuNmodelEntropy(4, entropy_dic)\n",
    "        self.entropy = entropy_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbb305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_files(path):\n",
    "    data_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            filename = os.path.join(root, file)\n",
    "            with open(filename, 'r', encoding='ANSI') as f:\n",
    "                txt = f.read()\n",
    "                txt = txt.replace('本书来自www.cr173.com免费txt小说下载站', '')\n",
    "                txt = txt.replace('更多更新免费电子书请关注www.cr173.com', '')\n",
    "                txt = txt.replace('〖新语丝电子文库(www.xys.org)〗', '')\n",
    "                d = ChineseData()\n",
    "                d.txt = txt\n",
    "                d.txtname = file.split('.')[0]\n",
    "                data_list.append(d)\n",
    "            f.close()\n",
    "    return data_list\n",
    "\n",
    "def read_punctuation_list(path):\n",
    "    punctuation = [line.strip() for line in open(path, encoding='UTF-8').readlines()]\n",
    "    punctuation.extend(['\\n', '\\u3000', '\\u0020', '\\u00A0'])\n",
    "    return punctuation\n",
    "\n",
    "def read_stopwords_list(path):\n",
    "    stopwords = [line.strip() for line in open(path, encoding='UTF-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb68f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:正在处理《三十三剑客图》...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\sonic\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache C:\\Users\\sonic\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.402 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.402 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n",
      "INFO:root:《三十三剑客图》处理完成...\n",
      "INFO:root:正在处理《书剑恩仇录》...\n",
      "INFO:root:《书剑恩仇录》处理完成...\n",
      "INFO:root:正在处理《侠客行》...\n",
      "INFO:root:《侠客行》处理完成...\n",
      "INFO:root:正在处理《倚天屠龙记》...\n",
      "INFO:root:《倚天屠龙记》处理完成...\n",
      "INFO:root:正在处理《天龙八部》...\n",
      "INFO:root:《天龙八部》处理完成...\n",
      "INFO:root:正在处理《射雕英雄传》...\n",
      "INFO:root:《射雕英雄传》处理完成...\n",
      "INFO:root:正在处理《白马啸西风》...\n",
      "INFO:root:《白马啸西风》处理完成...\n",
      "INFO:root:正在处理《碧血剑》...\n",
      "INFO:root:《碧血剑》处理完成...\n",
      "INFO:root:正在处理《神雕侠侣》...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_dir_path = './jyxstxtqj'\n",
    "    stopwords_path = './stopword/cn_stopwords.txt'\n",
    "    punctuation_path = './stopword/cn_punctuation.txt'\n",
    "#     global stopwords\n",
    "    stopwords = read_stopwords_list(stopwords_path)\n",
    "#     global punctuation\n",
    "    punctuation = read_punctuation_list(punctuation_path)\n",
    "    data_list = read_all_files(data_dir_path)\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        logging.info('正在处理《'+data_list[i].txtname+'》...')\n",
    "        ChineseData.run(data_list[i])\n",
    "        logging.info('《'+data_list[i].txtname+'》处理完成...')\n",
    "\n",
    "    np.save('data_list.npy', data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b519219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=np.load('data_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59232d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_results_sub(data):\n",
    "    k = len(data)\n",
    "    num = []\n",
    "    for i in range(k):\n",
    "        count = 0\n",
    "        for j in range(len(data[i].sentences)):\n",
    "            count += len(data[i].sentences[j])\n",
    "        num.append(count)\n",
    "    labels = []\n",
    "    for i in range(k):\n",
    "        labels.append(data[i].txtname + '\\n'  + str(num[i]))\n",
    "    entropylist = []\n",
    "    for i in range(4):\n",
    "        entropy = []\n",
    "        for j in range(k):\n",
    "            entropy.append(data[j].entropy[i + 1])\n",
    "        entropylist.append(entropy)\n",
    "\n",
    "    fonten = {'family': 'Times New Roman', 'size': 10}\n",
    "    \n",
    "    width = 0.3\n",
    "    ind = np.linspace(0.5, 0.5 + (k-1) , 1 + (k-1))\n",
    "    fig = plt.figure(1, figsize=(20, 10), dpi=300)\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax4 = fig.add_subplot(224)\n",
    "\n",
    "    ax1.bar(ind, entropylist[0], width, color='green')\n",
    "    ax2.bar(ind, entropylist[1], width, color='blue')\n",
    "    ax3.bar(ind, entropylist[2], width, color='yellow')\n",
    "    ax4.bar(ind, entropylist[3], width, color='red')\n",
    "\n",
    "    ax1.set_xticks(ind)\n",
    "    ax1.set_xticklabels(labels, size='small', rotation=40, fontweight='bold')\n",
    "    ax1.set_ylim(min(entropylist[0]) - 0.1, max(entropylist[0]) + 0.1)\n",
    "    ax1.set_ylabel('1-gram Average Entropy', fontdict=fonten, fontweight='bold')\n",
    "    for a, b in zip(ind, entropylist[0]):\n",
    "        ax1.text(a, b + 0.01, b, ha='center', va='bottom', fontdict=fonten)\n",
    "        \n",
    "    ax2.set_xticks(ind)\n",
    "    ax2.set_xticklabels(labels, size='small', rotation=40, fontweight='bold')\n",
    "    ax2.set_ylim(min(entropylist[1]) - 0.1, max(entropylist[1]) + 0.1)\n",
    "    ax2.set_ylabel('2-gram Average Entropy', fontdict=fonten, fontweight='bold')\n",
    "    for a, b in zip(ind, entropylist[1]):\n",
    "        ax2.text(a, b + 0.01, b, ha='center', va='bottom', fontdict=fonten)\n",
    "        \n",
    "    ax3.set_xticks(ind)\n",
    "    ax3.set_xticklabels(labels, size='small', rotation=40, fontweight='bold')\n",
    "    ax3.set_ylim(min(entropylist[2]) - 0.1, max(entropylist[2]) + 0.1)\n",
    "    ax3.set_ylabel('3-gram Average Entropy', fontdict=fonten, fontweight='bold')\n",
    "    for a, b in zip(ind, entropylist[2]):\n",
    "        ax3.text(a, b + 0.01, b, ha='center', va='bottom', fontdict=fonten)\n",
    "\n",
    "    ax4.set_xticks(ind)\n",
    "    ax4.set_xticklabels(labels, size='small', rotation=40, fontweight='bold')\n",
    "    ax4.set_ylim(max(min(entropylist[3]) - 0.1, 0), max(entropylist[3]) + 0.1)\n",
    "    ax4.set_ylabel('4-gram Average Entropy', fontdict=fonten, fontweight='bold')\n",
    "    for a, b in zip(ind, entropylist[3]):\n",
    "        ax4.text(a, b + 0.01, b, ha='center', va='bottom', fontdict=fonten)\n",
    "    \n",
    "    plt.savefig('chineseaverageentropy.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_results_sub(data_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
